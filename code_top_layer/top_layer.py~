import pickle
import numpy as np
import cv2
import theano
from network2 import Network,FullyConnectedLayer,SoftmaxLayer
import theano.tensor as T
from theano.tensor.nnet import softmax
from theano.tensor import shared_randomstreams
from theano.tensor import tanh

#### Constants
GPU = True
if GPU:
    print "Trying to run under a GPU.  If this is not desired, then modify "+\
        "code.py\nto set the GPU flag to False."
    try: theano.config.device = 'gpu'
    except: pass # it's already set
    theano.config.floatX = 'float32'
else:
    print "Running with a CPU.  If this is not desired, then the modify "+\
        "code.py to set\nthe GPU flag to True."

def shared(data):
        """Place the data into shared variables.  This allows Theano to copy
        the data to the GPU, if one is available.
        """
        shared_x = theano.shared(
            np.asarray(data, dtype=theano.config.floatX), borrow=True)
        return shared_x

net=Network([FullyConnectedLayer(6912,1024,activation_fn=tanh),SoftmaxLayer(1024,33)],10)
f1=file('train_top_layer.pkl','rb')
f2=file('validate_top_layer.pkl','rb')
f3=file('top_layer_model.pkl','wb')
epochs=20
mini_batch_size=30
eta=0.01
for i in range(200):
    descriptors,ground_distribution=pickle.load(f1)
    training_data=(shared(descriptors),shared(ground_distribution))
    desc,distribution=pickle.load(f2)
    validation_data=(shared(desc),shared(distribution))
    net.SGD(training_data, epochs, mini_batch_size,eta, validation_data)
pickle.dump(net,f3)
f1.close()
f2.close()
f3.close()
